{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../dataset/train/train.csv'\n",
    "test_dir = '../dataset/test/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset = pd.read_csv(train_dir)\n",
    "pd_dataset['sentence'].to_csv('data_sentence.txt', index=False, header=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "# Domain-pre-training corpora\n",
    "dpt_corpus_train = 'data_sentence.txt'\n",
    "dpt_corpus_train_data_selected = 'data_sentence_selected.txt'\n",
    "dpt_corpus_val = 'data_sentence_val.txt'\n",
    "\n",
    "# Fine-tuning corpora\n",
    "# If there are multiple downstream NLP tasks/corpora, you can concatenate those files together\n",
    "ft_corpus_train = 'vocab.txt'\n",
    "\n",
    "# Load Model & Tokenizer\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers_domain_adaptation import DataSelector\n",
    "\n",
    "selector = DataSelector(\n",
    "    keep=0.5,  # TODO Replace with `keep`\n",
    "    tokenizer=tokenizer,\n",
    "    similarity_metrics=['euclidean'],\n",
    "    diversity_metrics=[\n",
    "        \"type_token_ratio\",\n",
    "        \"entropy\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing similarity: 100%|██████████| 1/1 [00:26<00:00, 26.12s/metric]\n",
      "computing diversity: 100%|██████████| 2/2 [00:00<00:00,  2.03metric/s]\n"
     ]
    }
   ],
   "source": [
    "# Load text data into memory\n",
    "fine_tuning_texts = Path(ft_corpus_train).read_text(encoding='utf-8').splitlines()\n",
    "training_texts = Path(dpt_corpus_train).read_text(encoding='utf-8').splitlines()\n",
    "\n",
    "# Fit on fine-tuning corpus\n",
    "selector.fit(fine_tuning_texts)\n",
    "\n",
    "# Select relevant documents from in-domain training corpus\n",
    "selected_corpus = selector.transform(training_texts)\n",
    "\n",
    "# Save selected corpus to disk under `dpt_corpus_train_data_selected`\n",
    "Path(dpt_corpus_train_data_selected).write_text('\\n'.join(selected_corpus), encoding='utf-8');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"기원전 1400년에 발생한 다사라즈나 전투가 이들 인도아리아 부족들 사이에 발생한 전투들 중 제일 대표적인 전투로, 신흥 인도아리아인 부족인 바라타족이 수다스 왕의 통솔하에 펀자브 지역의 라비 강의 지배세력으로 크게 성장하자 푸루족의 왕인 푸루쿠트샤가 펀자브 지역의 다른 인도아리아인 9부족들의 왕들과 연합하여 바라타족을 공격하면서 다사라즈나 전투가 발발하였는데, 이 전투에서 수다스 왕의 통솔로 바라타족이 푸루족을 비롯한 인도아리아 10부족을 격파하며 전쟁에서 승리하면서 이들 10부족들은 바라타족에게 흡수되었으며, 이들 10개 부족을 흡수한 바라타족은 쿠루크셰트라 지역으로 이주하였고 이후 쿠루족이라는 인도아리아인 부족으로 발전하였다.\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers_domain_adaptation import VocabAugmentor\n",
    "\n",
    "target_vocab_size = 32500  # len(tokenizer) == 30_522\n",
    "\n",
    "augmentor = VocabAugmentor(\n",
    "    tokenizer=tokenizer, \n",
    "    cased=False,\n",
    "    target_vocab_size=target_vocab_size\n",
    ")\n",
    "\n",
    "# Obtain new domain-specific terminology based on the fine-tuning corpus\n",
    "#new_tokens = augmentor.get_new_tokens(ft_corpus_train)\n",
    "new_tokens = augmentor.get_new_tokens(open(dpt_corpus_train, 'rt', encoding='UTF8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model and tokenizer with new vocab terminologies\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Domain Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Union, Generator\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default-e7847afc364aadc7 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to C:\\Users\\N\\.cache\\huggingface\\datasets\\text\\default-e7847afc364aadc7\\0.0.0\\daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to C:\\Users\\N\\.cache\\huggingface\\datasets\\text\\default-e7847afc364aadc7\\0.0.0\\daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:01<00:00, 31.60ba/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 28.95ba/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\n",
    "    'text', \n",
    "    data_files={\n",
    "        \"train\": dpt_corpus_train, \n",
    "        \"val\": dpt_corpus_train_data_selected\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    lambda examples: tokenizer(examples['text'], truncation=True, max_length=model.config.max_position_embeddings), \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/domain_pre_training\",\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=40,\n",
    "    per_device_eval_batch_size=40,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    seed=42,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=2,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['val'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,  # This tokenizer has new tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "634b923af296264915c18b4811d0ef4120d1c0b994dd9741546346306f9698e8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
