{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= './dataset/train/train.csv'\n",
    "test_dir= './dataset/test/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "    subject_entity = [i[1:-1].split(',')[0].split(':')[1] for i in dataset['subject_entity']]\n",
    "    object_entity = [j[1:-1].split(',')[0].split(':')[1] for j in dataset['object_entity']]\n",
    "    out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})\n",
    "    \n",
    "    duplied = out_dataset[out_dataset.duplicated(subset=['sentence','subject_entity','object_entity'])]\n",
    "    duplied_no_idx = duplied[duplied['label'] == 'no_relation']['id'].to_list()\n",
    "    for idx in duplied_no_idx:\n",
    "        out_dataset.drop(out_dataset.loc[out_dataset['id']==idx].index, inplace=True)\n",
    "    out_dataset = out_dataset.drop_duplicates(subset=['sentence','subject_entity','object_entity','label'],keep='first')  \n",
    "    return out_dataset\n",
    "\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    return preprocessing_dataset(pd_dataset)\n",
    "\n",
    "\n",
    "def load_stratified_data(dataset_dir):\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "\n",
    "    for train_index, test_index in split.split(pd_dataset, pd_dataset[\"label\"]):\n",
    "        strat_train_set = pd_dataset.loc[train_index]\n",
    "        strat_dev_set = pd_dataset.loc[test_index]\n",
    "    train_dataset = preprocessing_dataset(strat_train_set)  \n",
    "    dev_dataset = preprocessing_dataset(strat_dev_set)  \n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('./dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset = load_stratified_data(train_dir)\n",
    "test_dataset = load_data(test_dir)\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'])\n",
    "dev_label = label_to_num(dev_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>10706</td>\n",
       "      <td>추승우(秋承佑, 1979년 9월 24일 ~)는 전 KBO 리그 한화 이글스의 외야수...</td>\n",
       "      <td>'추승우'</td>\n",
       "      <td>'외야수'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26891</th>\n",
       "      <td>26891</td>\n",
       "      <td>부안군에 따르면 부안군자원봉사센터와 전라북도자원봉사센터가 공동으로 주관한 이번 행사...</td>\n",
       "      <td>'부안군'</td>\n",
       "      <td>'전라북도'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29437</th>\n",
       "      <td>29437</td>\n",
       "      <td>중복인력 감축, 서울메트로, 서울특별시 도시철도공사 임원 인건비 절감으로 2027년...</td>\n",
       "      <td>'서울메트로'</td>\n",
       "      <td>'서울특별시'</td>\n",
       "      <td>org:member_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29780</th>\n",
       "      <td>29780</td>\n",
       "      <td>그 결과 민주정의당 대표인 노태우가 대통령 직선제 개헌을 수용하는 6·29선언이 발...</td>\n",
       "      <td>'민주정의당'</td>\n",
       "      <td>'노태우'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27887</th>\n",
       "      <td>27887</td>\n",
       "      <td>한편 창당에 앞서 옛 유신당의 대표였던 마쓰노 요리히사는 과거 민주당을 탈당한 전력...</td>\n",
       "      <td>'유신당'</td>\n",
       "      <td>'마쓰노 요리히사'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31784</th>\n",
       "      <td>31784</td>\n",
       "      <td>모스크바 시간 기준 2015년 5월 12일, 암살당한 야당인사 보리스 넴초프가 한때...</td>\n",
       "      <td>'자유당'</td>\n",
       "      <td>'자유주의'</td>\n",
       "      <td>org:political/religious_affiliation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22143</th>\n",
       "      <td>22143</td>\n",
       "      <td>김범수(金範洙, 1979년 1월 26일 ~)는 대한민국의 가수이다.</td>\n",
       "      <td>'김범수'</td>\n",
       "      <td>'1979년 1월 26일'</td>\n",
       "      <td>per:date_of_birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8825</th>\n",
       "      <td>8825</td>\n",
       "      <td>일반적으로 김정은의 어머니는 고용희(고영희)인 것으로 알려지고 있지만, 이복형 김정...</td>\n",
       "      <td>'고용희'</td>\n",
       "      <td>'김정은'</td>\n",
       "      <td>per:children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20549</th>\n",
       "      <td>20549</td>\n",
       "      <td>구자욱의 타구를 잡은 1루수 제이미 로맥이 1루를 찍은 뒤 2루로 뛰던 1루 주자를...</td>\n",
       "      <td>'제이미 로맥'</td>\n",
       "      <td>'1루수'</td>\n",
       "      <td>per:title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20244</th>\n",
       "      <td>20244</td>\n",
       "      <td>광주시는 한국승강기안전공단 호남지역본부와 지난 4월 업무협약을 체결하고 승강기 안전...</td>\n",
       "      <td>'한국승강기안전공단'</td>\n",
       "      <td>'승강기 안전'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25946 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "10706  10706  추승우(秋承佑, 1979년 9월 24일 ~)는 전 KBO 리그 한화 이글스의 외야수...   \n",
       "26891  26891  부안군에 따르면 부안군자원봉사센터와 전라북도자원봉사센터가 공동으로 주관한 이번 행사...   \n",
       "29437  29437  중복인력 감축, 서울메트로, 서울특별시 도시철도공사 임원 인건비 절감으로 2027년...   \n",
       "29780  29780  그 결과 민주정의당 대표인 노태우가 대통령 직선제 개헌을 수용하는 6·29선언이 발...   \n",
       "27887  27887  한편 창당에 앞서 옛 유신당의 대표였던 마쓰노 요리히사는 과거 민주당을 탈당한 전력...   \n",
       "...      ...                                                ...   \n",
       "31784  31784  모스크바 시간 기준 2015년 5월 12일, 암살당한 야당인사 보리스 넴초프가 한때...   \n",
       "22143  22143              김범수(金範洙, 1979년 1월 26일 ~)는 대한민국의 가수이다.   \n",
       "8825    8825  일반적으로 김정은의 어머니는 고용희(고영희)인 것으로 알려지고 있지만, 이복형 김정...   \n",
       "20549  20549  구자욱의 타구를 잡은 1루수 제이미 로맥이 1루를 찍은 뒤 2루로 뛰던 1루 주자를...   \n",
       "20244  20244  광주시는 한국승강기안전공단 호남지역본부와 지난 4월 업무협약을 체결하고 승강기 안전...   \n",
       "\n",
       "      subject_entity    object_entity                                label  \n",
       "10706          '추승우'            '외야수'                          no_relation  \n",
       "26891          '부안군'           '전라북도'                          no_relation  \n",
       "29437        '서울메트로'          '서울특별시'                        org:member_of  \n",
       "29780        '민주정의당'            '노태우'            org:top_members/employees  \n",
       "27887          '유신당'       '마쓰노 요리히사'            org:top_members/employees  \n",
       "...              ...              ...                                  ...  \n",
       "31784          '자유당'           '자유주의'  org:political/religious_affiliation  \n",
       "22143          '김범수'   '1979년 1월 26일'                    per:date_of_birth  \n",
       "8825           '고용희'            '김정은'                         per:children  \n",
       "20549       '제이미 로맥'            '1루수'                            per:title  \n",
       "20244    '한국승강기안전공단'         '승강기 안전'                          no_relation  \n",
       "\n",
       "[25946 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      )\n",
    "  return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "tokenized_test = tokenized_dataset(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,    11,  1672,  ...,     1,     1,     1],\n",
       "        [    0,    11, 22902,  ...,     1,     1,     1],\n",
       "        [    0,    11,  3671,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,    11,  4571,  ...,     1,     1,     1],\n",
       "        [    0,    11, 10258,  ...,     1,     1,     1],\n",
       "        [    0,    11,  3629,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 30\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "                  'org:product', 'per:title', 'org:alternate_names',\n",
    "                  'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "                  'org:number_of_employees/members', 'per:children',\n",
    "                  'per:place_of_residence', 'per:alternate_names',\n",
    "                  'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "                  'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "                  'org:member_of', 'per:parents', 'org:dissolved',\n",
    "                  'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "                  'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "                  'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\" validation을 위한 metrics function \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = pred.predictions\n",
    "\n",
    "    # calculate accuracy using sklearn's function\n",
    "    f1 = klue_re_micro_f1(preds, labels)\n",
    "    auprc = klue_re_auprc(probs, labels)\n",
    "    acc = accuracy_score(labels, preds)  # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "    return {'micro f1 score': f1, 'auprc': auprc, 'accuracy': acc,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_everything(2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    save_total_limit=5,              # number of total save model.\n",
    "    save_steps=100,                 # model saving step.\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    learning_rate=5e-5,               # learning_rate\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,              # log saving step.\n",
    "    evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                                # `no`: No evaluation during training.\n",
    "                                # `steps`: Evaluate every `eval_steps`.\n",
    "                                # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps = 100,            # evaluation step.\n",
    "    load_best_model_at_end = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 25946\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 77838\n",
      "  0%|          | 15/77838 [01:35<139:19:03,  6.44s/it]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=RE_train_dataset,         # training dataset\n",
    "        eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "        compute_metrics=compute_metrics,         # define metrics function\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3436d0af85bf8ce2a04a3a6387bb490cfbe128d8ca263d1c66e02d80c8625091"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('home': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
