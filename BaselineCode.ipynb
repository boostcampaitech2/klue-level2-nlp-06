{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir= './dataset/train/train.csv'\n",
    "test_dir= './dataset/test/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\" ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½ ì‹œì¼œì¤ë‹ˆë‹¤.\"\"\"\n",
    "    subject_entity = [i[1:-1].split(',')[0].split(':')[1] for i in dataset['subject_entity']]\n",
    "    object_entity = [j[1:-1].split(',')[0].split(':')[1] for j in dataset['object_entity']]\n",
    "    out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,'object_entity':object_entity,'label':dataset['label'],})\n",
    "    \n",
    "    duplied = out_dataset[out_dataset.duplicated(subset=['sentence','subject_entity','object_entity'])]\n",
    "    duplied_no_idx = duplied[duplied['label'] == 'no_relation']['id'].to_list()\n",
    "    for idx in duplied_no_idx:\n",
    "        out_dataset.drop(out_dataset.loc[out_dataset['id']==idx].index, inplace=True)\n",
    "    out_dataset = out_dataset.drop_duplicates(subset=['sentence','subject_entity','object_entity','label'],keep='first')  \n",
    "    return out_dataset\n",
    "\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    \"\"\" csv íŒŒì¼ì„ ê²½ë¡œì— ë§¡ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤. \"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    return preprocessing_dataset(pd_dataset)\n",
    "\n",
    "\n",
    "def load_stratified_data(dataset_dir):\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "\n",
    "    for train_index, test_index in split.split(pd_dataset, pd_dataset[\"label\"]):\n",
    "        strat_train_set = pd_dataset.loc[train_index]\n",
    "        strat_dev_set = pd_dataset.loc[test_index]\n",
    "    train_dataset = preprocessing_dataset(strat_train_set)  \n",
    "    dev_dataset = preprocessing_dataset(strat_dev_set)  \n",
    "    return train_dataset, dev_dataset\n",
    "\n",
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open('./dict_label_to_num.pkl', 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset = load_stratified_data(train_dir)\n",
    "test_dataset = load_data(test_dir)\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'])\n",
    "dev_label = label_to_num(dev_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10706</th>\n",
       "      <td>10706</td>\n",
       "      <td>ì¶”ìŠ¹ìš°(ç§‹æ‰¿ä½‘, 1979ë…„ 9ì›” 24ì¼ ~)ëŠ” ì „ KBO ë¦¬ê·¸ í•œí™” ì´ê¸€ìŠ¤ì˜ ì™¸ì•¼ìˆ˜...</td>\n",
       "      <td>'ì¶”ìŠ¹ìš°'</td>\n",
       "      <td>'ì™¸ì•¼ìˆ˜'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26891</th>\n",
       "      <td>26891</td>\n",
       "      <td>ë¶€ì•ˆêµ°ì— ë”°ë¥´ë©´ ë¶€ì•ˆêµ°ìì›ë´‰ì‚¬ì„¼í„°ì™€ ì „ë¼ë¶ë„ìì›ë´‰ì‚¬ì„¼í„°ê°€ ê³µë™ìœ¼ë¡œ ì£¼ê´€í•œ ì´ë²ˆ í–‰ì‚¬...</td>\n",
       "      <td>'ë¶€ì•ˆêµ°'</td>\n",
       "      <td>'ì „ë¼ë¶ë„'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29437</th>\n",
       "      <td>29437</td>\n",
       "      <td>ì¤‘ë³µì¸ë ¥ ê°ì¶•, ì„œìš¸ë©”íŠ¸ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œ ë„ì‹œì² ë„ê³µì‚¬ ì„ì› ì¸ê±´ë¹„ ì ˆê°ìœ¼ë¡œ 2027ë…„...</td>\n",
       "      <td>'ì„œìš¸ë©”íŠ¸ë¡œ'</td>\n",
       "      <td>'ì„œìš¸íŠ¹ë³„ì‹œ'</td>\n",
       "      <td>org:member_of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29780</th>\n",
       "      <td>29780</td>\n",
       "      <td>ê·¸ ê²°ê³¼ ë¯¼ì£¼ì •ì˜ë‹¹ ëŒ€í‘œì¸ ë…¸íƒœìš°ê°€ ëŒ€í†µë ¹ ì§ì„ ì œ ê°œí—Œì„ ìˆ˜ìš©í•˜ëŠ” 6Â·29ì„ ì–¸ì´ ë°œ...</td>\n",
       "      <td>'ë¯¼ì£¼ì •ì˜ë‹¹'</td>\n",
       "      <td>'ë…¸íƒœìš°'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27887</th>\n",
       "      <td>27887</td>\n",
       "      <td>í•œí¸ ì°½ë‹¹ì— ì•ì„œ ì˜› ìœ ì‹ ë‹¹ì˜ ëŒ€í‘œì˜€ë˜ ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬ëŠ” ê³¼ê±° ë¯¼ì£¼ë‹¹ì„ íƒˆë‹¹í•œ ì „ë ¥...</td>\n",
       "      <td>'ìœ ì‹ ë‹¹'</td>\n",
       "      <td>'ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬'</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31784</th>\n",
       "      <td>31784</td>\n",
       "      <td>ëª¨ìŠ¤í¬ë°” ì‹œê°„ ê¸°ì¤€ 2015ë…„ 5ì›” 12ì¼, ì•”ì‚´ë‹¹í•œ ì•¼ë‹¹ì¸ì‚¬ ë³´ë¦¬ìŠ¤ ë„´ì´ˆí”„ê°€ í•œë•Œ...</td>\n",
       "      <td>'ììœ ë‹¹'</td>\n",
       "      <td>'ììœ ì£¼ì˜'</td>\n",
       "      <td>org:political/religious_affiliation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22143</th>\n",
       "      <td>22143</td>\n",
       "      <td>ê¹€ë²”ìˆ˜(é‡‘ç¯„æ´™, 1979ë…„ 1ì›” 26ì¼ ~)ëŠ” ëŒ€í•œë¯¼êµ­ì˜ ê°€ìˆ˜ì´ë‹¤.</td>\n",
       "      <td>'ê¹€ë²”ìˆ˜'</td>\n",
       "      <td>'1979ë…„ 1ì›” 26ì¼'</td>\n",
       "      <td>per:date_of_birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8825</th>\n",
       "      <td>8825</td>\n",
       "      <td>ì¼ë°˜ì ìœ¼ë¡œ ê¹€ì •ì€ì˜ ì–´ë¨¸ë‹ˆëŠ” ê³ ìš©í¬(ê³ ì˜í¬)ì¸ ê²ƒìœ¼ë¡œ ì•Œë ¤ì§€ê³  ìˆì§€ë§Œ, ì´ë³µí˜• ê¹€ì •...</td>\n",
       "      <td>'ê³ ìš©í¬'</td>\n",
       "      <td>'ê¹€ì •ì€'</td>\n",
       "      <td>per:children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20549</th>\n",
       "      <td>20549</td>\n",
       "      <td>êµ¬ììš±ì˜ íƒ€êµ¬ë¥¼ ì¡ì€ 1ë£¨ìˆ˜ ì œì´ë¯¸ ë¡œë§¥ì´ 1ë£¨ë¥¼ ì°ì€ ë’¤ 2ë£¨ë¡œ ë›°ë˜ 1ë£¨ ì£¼ìë¥¼...</td>\n",
       "      <td>'ì œì´ë¯¸ ë¡œë§¥'</td>\n",
       "      <td>'1ë£¨ìˆ˜'</td>\n",
       "      <td>per:title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20244</th>\n",
       "      <td>20244</td>\n",
       "      <td>ê´‘ì£¼ì‹œëŠ” í•œêµ­ìŠ¹ê°•ê¸°ì•ˆì „ê³µë‹¨ í˜¸ë‚¨ì§€ì—­ë³¸ë¶€ì™€ ì§€ë‚œ 4ì›” ì—…ë¬´í˜‘ì•½ì„ ì²´ê²°í•˜ê³  ìŠ¹ê°•ê¸° ì•ˆì „...</td>\n",
       "      <td>'í•œêµ­ìŠ¹ê°•ê¸°ì•ˆì „ê³µë‹¨'</td>\n",
       "      <td>'ìŠ¹ê°•ê¸° ì•ˆì „'</td>\n",
       "      <td>no_relation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25946 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           sentence  \\\n",
       "10706  10706  ì¶”ìŠ¹ìš°(ç§‹æ‰¿ä½‘, 1979ë…„ 9ì›” 24ì¼ ~)ëŠ” ì „ KBO ë¦¬ê·¸ í•œí™” ì´ê¸€ìŠ¤ì˜ ì™¸ì•¼ìˆ˜...   \n",
       "26891  26891  ë¶€ì•ˆêµ°ì— ë”°ë¥´ë©´ ë¶€ì•ˆêµ°ìì›ë´‰ì‚¬ì„¼í„°ì™€ ì „ë¼ë¶ë„ìì›ë´‰ì‚¬ì„¼í„°ê°€ ê³µë™ìœ¼ë¡œ ì£¼ê´€í•œ ì´ë²ˆ í–‰ì‚¬...   \n",
       "29437  29437  ì¤‘ë³µì¸ë ¥ ê°ì¶•, ì„œìš¸ë©”íŠ¸ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œ ë„ì‹œì² ë„ê³µì‚¬ ì„ì› ì¸ê±´ë¹„ ì ˆê°ìœ¼ë¡œ 2027ë…„...   \n",
       "29780  29780  ê·¸ ê²°ê³¼ ë¯¼ì£¼ì •ì˜ë‹¹ ëŒ€í‘œì¸ ë…¸íƒœìš°ê°€ ëŒ€í†µë ¹ ì§ì„ ì œ ê°œí—Œì„ ìˆ˜ìš©í•˜ëŠ” 6Â·29ì„ ì–¸ì´ ë°œ...   \n",
       "27887  27887  í•œí¸ ì°½ë‹¹ì— ì•ì„œ ì˜› ìœ ì‹ ë‹¹ì˜ ëŒ€í‘œì˜€ë˜ ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬ëŠ” ê³¼ê±° ë¯¼ì£¼ë‹¹ì„ íƒˆë‹¹í•œ ì „ë ¥...   \n",
       "...      ...                                                ...   \n",
       "31784  31784  ëª¨ìŠ¤í¬ë°” ì‹œê°„ ê¸°ì¤€ 2015ë…„ 5ì›” 12ì¼, ì•”ì‚´ë‹¹í•œ ì•¼ë‹¹ì¸ì‚¬ ë³´ë¦¬ìŠ¤ ë„´ì´ˆí”„ê°€ í•œë•Œ...   \n",
       "22143  22143              ê¹€ë²”ìˆ˜(é‡‘ç¯„æ´™, 1979ë…„ 1ì›” 26ì¼ ~)ëŠ” ëŒ€í•œë¯¼êµ­ì˜ ê°€ìˆ˜ì´ë‹¤.   \n",
       "8825    8825  ì¼ë°˜ì ìœ¼ë¡œ ê¹€ì •ì€ì˜ ì–´ë¨¸ë‹ˆëŠ” ê³ ìš©í¬(ê³ ì˜í¬)ì¸ ê²ƒìœ¼ë¡œ ì•Œë ¤ì§€ê³  ìˆì§€ë§Œ, ì´ë³µí˜• ê¹€ì •...   \n",
       "20549  20549  êµ¬ììš±ì˜ íƒ€êµ¬ë¥¼ ì¡ì€ 1ë£¨ìˆ˜ ì œì´ë¯¸ ë¡œë§¥ì´ 1ë£¨ë¥¼ ì°ì€ ë’¤ 2ë£¨ë¡œ ë›°ë˜ 1ë£¨ ì£¼ìë¥¼...   \n",
       "20244  20244  ê´‘ì£¼ì‹œëŠ” í•œêµ­ìŠ¹ê°•ê¸°ì•ˆì „ê³µë‹¨ í˜¸ë‚¨ì§€ì—­ë³¸ë¶€ì™€ ì§€ë‚œ 4ì›” ì—…ë¬´í˜‘ì•½ì„ ì²´ê²°í•˜ê³  ìŠ¹ê°•ê¸° ì•ˆì „...   \n",
       "\n",
       "      subject_entity    object_entity                                label  \n",
       "10706          'ì¶”ìŠ¹ìš°'            'ì™¸ì•¼ìˆ˜'                          no_relation  \n",
       "26891          'ë¶€ì•ˆêµ°'           'ì „ë¼ë¶ë„'                          no_relation  \n",
       "29437        'ì„œìš¸ë©”íŠ¸ë¡œ'          'ì„œìš¸íŠ¹ë³„ì‹œ'                        org:member_of  \n",
       "29780        'ë¯¼ì£¼ì •ì˜ë‹¹'            'ë…¸íƒœìš°'            org:top_members/employees  \n",
       "27887          'ìœ ì‹ ë‹¹'       'ë§ˆì“°ë…¸ ìš”ë¦¬íˆì‚¬'            org:top_members/employees  \n",
       "...              ...              ...                                  ...  \n",
       "31784          'ììœ ë‹¹'           'ììœ ì£¼ì˜'  org:political/religious_affiliation  \n",
       "22143          'ê¹€ë²”ìˆ˜'   '1979ë…„ 1ì›” 26ì¼'                    per:date_of_birth  \n",
       "8825           'ê³ ìš©í¬'            'ê¹€ì •ì€'                         per:children  \n",
       "20549       'ì œì´ë¯¸ ë¡œë§¥'            '1ë£¨ìˆ˜'                            per:title  \n",
       "20244    'í•œêµ­ìŠ¹ê°•ê¸°ì•ˆì „ê³µë‹¨'         'ìŠ¹ê°•ê¸° ì•ˆì „'                          no_relation  \n",
       "\n",
       "[25946 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenizier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  \"\"\" tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤.\"\"\"\n",
    "  concat_entity = []\n",
    "  for e01, e02 in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "    temp = ''\n",
    "    temp = e01 + '[SEP]' + e02\n",
    "    concat_entity.append(temp)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      return_token_type_ids=False,\n",
    "      )\n",
    "  return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "tokenized_test = tokenized_dataset(test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,    11,  1672,  ...,     1,     1,     1],\n",
       "        [    0,    11, 22902,  ...,     1,     1,     1],\n",
       "        [    0,    11,  3671,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,    11,  4571,  ...,     1,     1,     1],\n",
       "        [    0,    11, 10258,  ...,     1,     1,     1],\n",
       "        [    0,    11,  3629,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset êµ¬ì„±ì„ ìœ„í•œ class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "model_config.num_labels = 30\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['no_relation', 'org:top_members/employees', 'org:members',\n",
    "                  'org:product', 'per:title', 'org:alternate_names',\n",
    "                  'per:employee_of', 'org:place_of_headquarters', 'per:product',\n",
    "                  'org:number_of_employees/members', 'per:children',\n",
    "                  'per:place_of_residence', 'per:alternate_names',\n",
    "                  'per:other_family', 'per:colleagues', 'per:origin', 'per:siblings',\n",
    "                  'per:spouse', 'org:founded', 'org:political/religious_affiliation',\n",
    "                  'org:member_of', 'per:parents', 'org:dissolved',\n",
    "                  'per:schools_attended', 'per:date_of_death', 'per:date_of_birth',\n",
    "                  'per:place_of_birth', 'per:place_of_death', 'org:founded_by',\n",
    "                  'per:religion']\n",
    "    no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(30)[labels]\n",
    "\n",
    "    score = np.zeros((30,))\n",
    "    for c in range(30):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\" validationì„ ìœ„í•œ metrics function \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = pred.predictions\n",
    "\n",
    "    # calculate accuracy using sklearn's function\n",
    "    f1 = klue_re_micro_f1(preds, labels)\n",
    "    auprc = klue_re_auprc(probs, labels)\n",
    "    acc = accuracy_score(labels, preds)  # ë¦¬ë”ë³´ë“œ í‰ê°€ì—ëŠ” í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "    return {'micro f1 score': f1, 'auprc': auprc, 'accuracy': acc,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed_everything(2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    save_total_limit=5,              # number of total save model.\n",
    "    save_steps=100,                 # model saving step.\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    learning_rate=5e-5,               # learning_rate\n",
    "    per_device_train_batch_size=1,  # batch size per device during training\n",
    "    per_device_eval_batch_size=1,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=100,              # log saving step.\n",
    "    evaluation_strategy='steps', # evaluation strategy to adopt during training\n",
    "                                # `no`: No evaluation during training.\n",
    "                                # `steps`: Evaluate every `eval_steps`.\n",
    "                                # `epoch`: Evaluate every end of epoch.\n",
    "    eval_steps = 100,            # evaluation step.\n",
    "    load_best_model_at_end = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 25946\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 77838\n",
      "  0%|          | 15/77838 [01:35<139:19:03,  6.44s/it]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,                         # the instantiated ğŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=RE_train_dataset,         # training dataset\n",
    "        eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "        compute_metrics=compute_metrics,         # define metrics function\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3436d0af85bf8ce2a04a3a6387bb490cfbe128d8ca263d1c66e02d80c8625091"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('home': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
